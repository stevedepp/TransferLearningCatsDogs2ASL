{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_keras_custom_classifier_with_transfer_learning_100_epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevedepp/TransferLearningCatsDogs2ASL/blob/main/1_keras_custom_classifier_with_transfer_learning_100_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFEmlOEap5gf"
      },
      "source": [
        "**1_keras_custom_classifier_with_transfer_learning_100_epochs.ipynb**\n",
        "\n",
        "This version runs the model 100 epochs on the hands data which is sourced from my google drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxvYFwlf9oHE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-3/1-keras-custom-classifier-with-transfer-learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-3/1-keras-custom-classifier-with-transfer-learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "This code is part of [Chapter 3: Cats versus Dogs: Transfer Learning in 30 Lines with Keras](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch03.html).\n",
        "\n",
        "Note: In order to run this notebook on Google Colab you need to [follow these instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#scrollTo=WzIRIt9d2huC) so that the local data such as the images are available in your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLck_0KI9oHF"
      },
      "source": [
        "# Building a Custom Classifier in Keras with Transfer Learning\n",
        "\n",
        "As promised, it’s time to build our state of the art classifier in 30 lines or fewer! At a high level, we will follow the steps shown below:\n",
        "\n",
        "- **Organize the data**: Download labeled images of cats and dogs from Kaggle. Then divide the images into training and validation folders.\n",
        "- **Set up the configuration**: Define a pipeline for reading data, including preprocessing the images (e.g. resizing) and batching multiple images together.\n",
        "- **Load and augment the data**: In the absence of a ton of training images, make small changes (augmentation) like rotation, zooming, etc to increase variation in training data.\n",
        "- **Define the model**: Take a pre-trained model, remove the last few layers, and append a new classifier layer. Freeze the weights of original layers (i.e. make them unmodifiable). Select an optimizer algorithm and a metric to track (like accuracy).\n",
        "- **Train and test**: Start training for a few iterations. Save the model to eventually load inside any application for predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhb_YlzF9oHG"
      },
      "source": [
        "## Organize the data\n",
        "\n",
        "Before training, we need to store our [downloaded dataset](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/train.zip) in the right folder structure. Remember to make the `data` directory where we will be performing the refactoring. We’ll divide the images into two sets – training and validation. Our directory structure will look something like this: \n",
        "\n",
        "```\n",
        "data\n",
        " |__train\n",
        " |    |__cat\n",
        " |    |__dog\n",
        " |__val\n",
        "      |__cat\n",
        "      |__dog\n",
        "```\n",
        "\n",
        "In Linux/Mac, the following lines of command can help achieve this directory structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEhPyNnlPwHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9f478d61-1e5e-497d-dbd5-b448c938abfe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMGjzh1eSZGA"
      },
      "source": [
        "path_data = '/content/gdrive/My Drive/data/dog_cat/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FD8_zw9-boa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6326b48e-b154-45ff-c7b2-756b79556525"
      },
      "source": [
        "!ls -l /content/gdrive/My\\ Drive/data/dog_cat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwx------ 2 root root 4096 Jun  3 23:32 train\n",
            "drwx------ 2 root root 4096 Jun  3 23:32 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc4V1boX9oHH"
      },
      "source": [
        "#!unzip train.zip\n",
        "#%mv train data\n",
        "#%cd data\n",
        "#%mkdir train val\n",
        "#%mkdir train/cat train/dog\n",
        "#%mkdir val/cat val/dog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhrx_r7-9oHM"
      },
      "source": [
        "The 25,000 files inside the data folder are prefixed with ‘cat’ and ‘dog’. Now, move the files into their respective directories. To keep our initial experiment short, we’ll pick 250 random files per class and place them in training and validation folders. You can increase/decrease this number anytime, to experiment with a trade-off between accuracy and speed. \n",
        "\n",
        "Classification accuracy on previously unseen images (in the validation folder) is a good proxy for how the classifier would perform in the real world. Ideally, the more training images, the better the learning will be. And, the more validation images, the better our classifier would perform in the real-world."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBG3ABDW9oHN"
      },
      "source": [
        "#!ls | grep cat | sort -R | head -250 | xargs -I {} mv {} train/cat/\n",
        "#!ls | grep dog | sort -R | head -250 | xargs -I {} mv {} train/dog/\n",
        "#!ls | grep cat | sort -R | head -250 | xargs -I {} mv {} val/cat/\n",
        "#!ls | grep dog | sort -R | head -250 | xargs -I {} mv {} val/dog/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnFLSTj9oHQ"
      },
      "source": [
        "## Set up the configuration\n",
        "\n",
        "Let's start off with our Python program and begin with importing the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiv0g2BL9oHR"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NF8ca-49oHU"
      },
      "source": [
        "Let's place all the configurations up-front. These can be modified in the future based on the dataset of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWp4dsAvk_S_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "585254ca-551c-4108-eb2c-5a6989589504"
      },
      "source": [
        "TRAIN_DATA_DIR = path_data + 'train/'\n",
        "TRAIN_DATA_DIR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/data/dog_cat/train/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxc8nPdz9oHV"
      },
      "source": [
        "TRAIN_DATA_DIR = path_data + 'train/'\n",
        "VALIDATION_DATA_DIR = path_data + 'val/'\n",
        "TRAIN_SAMPLES = 500\n",
        "VALIDATION_SAMPLES = 500\n",
        "NUM_CLASSES = 2\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLV_3Ag9oHY"
      },
      "source": [
        "## Load and augment the data\n",
        "\n",
        "Colored images usually have 3 channels viz. red, green and blue, each with intensity value ranging from 0 to 255. To normalize it (i.e. bring the value between 0 to 1), we can rescale the image by dividing each pixel by 255. Or, we can use the default `preprocess_input` function in Keras which does the preprocessing for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5F7_5Bl9oHZ"
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   zoom_range=0.2)\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSA6j_sS9oHc"
      },
      "source": [
        "Time to load the data from its directories and let the augmentation happen! \n",
        "\n",
        "A few key things to note:\n",
        "\n",
        "- Training one image at a time can be pretty inefficient, so we can batch them into groups. \n",
        "- To introduce more randomness during the training process, we’ll keep shuffling the images in each batch.\n",
        "- To bring reproducibility during multiple runs of the same program, we’ll give the random number generator a seed value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFkWynbE9oHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2e6ae384-9ea0-4588-e798-3030ab3d5d0e"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(TRAIN_DATA_DIR,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    batch_size=BATCH_SIZE,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=12345,\n",
        "                                                    class_mode='categorical')\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    VALIDATION_DATA_DIR,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcgq98O9oHg"
      },
      "source": [
        "Now that the data is taken care of, we come to the most crucial component of our training process - the model. We will reuse a CNN previously trained on the ImageNet dataset, remove the ImageNet specific classifier in the last few layers, and replace it with our own classifier suited to our problem. For transfer learning, we’ll ‘freeze’ the weights of the original model, i.e. set those layers as unmodifiable, so only the layers of the new classifier (that we add) can be modified. To keep things fast, we’ll choose the MobileNet model. Don’t worry about the specific layers, we’ll dig deeper into those details in [Chapter 4](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch04.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBioEBHW9oHh"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbXgtX4r9oHi"
      },
      "source": [
        "def model_maker():\n",
        "    base_model = MobileNet(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYjWxUxh9oHl"
      },
      "source": [
        "## Train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Vj3ERl9oHm",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "00be7bf6-9674-4f4f-aabb-aa79d5c05eb3"
      },
      "source": [
        "model = model_maker()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / BATCH_SIZE),\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=math.ceil(float(VALIDATION_SAMPLES) / BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From <ipython-input-35-f99b0fc6fbc9>:10: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 387s 48s/step - loss: 0.5332 - acc: 0.7398 - val_loss: 0.0915 - val_acc: 0.9707\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 91s 11s/step - loss: 0.2089 - acc: 0.9160 - val_loss: 0.0344 - val_acc: 0.9863\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 49s 6s/step - loss: 0.1624 - acc: 0.9473 - val_loss: 0.0545 - val_acc: 0.9785\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 22s 3s/step - loss: 0.1270 - acc: 0.9611 - val_loss: 0.0357 - val_acc: 0.9863\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.1567 - acc: 0.9365 - val_loss: 0.0494 - val_acc: 0.9824\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.0899 - acc: 0.9611 - val_loss: 0.0415 - val_acc: 0.9844\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0735 - acc: 0.9688 - val_loss: 0.0255 - val_acc: 0.9902\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0852 - acc: 0.9746 - val_loss: 0.0774 - val_acc: 0.9648\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0959 - acc: 0.9631 - val_loss: 0.0133 - val_acc: 0.9941\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0986 - acc: 0.9570 - val_loss: 0.0567 - val_acc: 0.9746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff2a9a466a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnEiBte9oHp"
      },
      "source": [
        "On our runs, it took was 5 seconds in the very first epoch to reach 90% accuracy on the validation set, with just 500 training images. Whoa! And by the 10th step, we observe about 97% validation accuracy. That’s the power of transfer learning. \n",
        "\n",
        "Without having the model previously trained on ImageNet, getting a decent accuracy on this task would have taken (1) training time anywhere between a couple of hours to a few days (2) tons of more data to get decent results.\n",
        "\n",
        "Before we forget, save the model you trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzX38yRr9oHq"
      },
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bk9DNwb9oHt"
      },
      "source": [
        "## Model Prediction\n",
        "\n",
        "Now that you have a trained model, you might eventually want to use it later for your application. We can now load this model anytime and classify an image. The Keras function `load_model`, as the name suggests loads the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0LPnGh49oHu"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtRZ0pQ59oHx"
      },
      "source": [
        "Now let’s try loading our original sample images and see what results we get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM9BgI7s9oHy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "28033228-4ba0-4c53-ddac-d84595d67bc7"
      },
      "source": [
        "img_path = path_data+'dog.jpeg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
        "prediction = model.predict(preprocessed_img)\n",
        "print(prediction)\n",
        "print(validation_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00272785 0.9972722 ]]\n",
            "{'cat': 0, 'dog': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1rvB3Gt5bm7"
      },
      "source": [
        "## Hands\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcY7Dg1A5aD5"
      },
      "source": [
        "path_data = '/content/gdrive/My Drive/data/hands/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSgjXTbo5tus"
      },
      "source": [
        "TRAIN_DATA_DIR = path_data + 'train/'\n",
        "VALIDATION_DATA_DIR = path_data + 'val/'\n",
        "TRAIN_SAMPLES = 200\n",
        "VALIDATION_SAMPLES = 200\n",
        "NUM_CLASSES = 26\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQmTPgPm5yOf"
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   zoom_range=0.2)\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJDhjbh552qb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b70cb899-5821-4c72-e0b9-0b3ce4e6d7e3"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(TRAIN_DATA_DIR,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    batch_size=BATCH_SIZE,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=12345,\n",
        "                                                    class_mode='categorical')\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    VALIDATION_DATA_DIR,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4680 images belonging to 26 classes.\n",
            "Found 520 images belonging to 26 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkrB9_Gz6Eet"
      },
      "source": [
        "def model_maker():\n",
        "    base_model = MobileNet(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NWtGRagHjnQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "75f3ec43-bd29-43b3-c2d5-3573a18bb930"
      },
      "source": [
        "model = model_maker()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / BATCH_SIZE),\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=math.ceil(float(VALIDATION_SAMPLES) / BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 208s 52s/step - loss: 3.7207 - acc: 0.0391 - val_loss: 3.2696 - val_acc: 0.1094\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 73s 18s/step - loss: 3.3290 - acc: 0.0625 - val_loss: 3.1038 - val_acc: 0.1875\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 68s 17s/step - loss: 3.1925 - acc: 0.0781 - val_loss: 3.0414 - val_acc: 0.1602\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 67s 17s/step - loss: 3.1212 - acc: 0.0703 - val_loss: 2.9139 - val_acc: 0.1797\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 64s 16s/step - loss: 3.0036 - acc: 0.1484 - val_loss: 2.8644 - val_acc: 0.1367\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 54s 13s/step - loss: 2.9669 - acc: 0.1250 - val_loss: 2.7418 - val_acc: 0.2148\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 50s 13s/step - loss: 2.8942 - acc: 0.1172 - val_loss: 2.6111 - val_acc: 0.3711\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 57s 14s/step - loss: 2.9319 - acc: 0.1250 - val_loss: 2.5694 - val_acc: 0.4805\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 45s 11s/step - loss: 2.7573 - acc: 0.2344 - val_loss: 2.4406 - val_acc: 0.5898\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 47s 12s/step - loss: 2.7125 - acc: 0.2148 - val_loss: 2.2980 - val_acc: 0.5898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff2b2ec36d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHpbG9AzO2vw"
      },
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96SpkCSRO2zL"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9cZs_UtO216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1624d8bf-538c-483c-9d64-04dfac39383b"
      },
      "source": [
        "img_path = path_data+'A538.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
        "prediction = model.predict(preprocessed_img)\n",
        "print(prediction)\n",
        "print(validation_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.03139458 0.0719717  0.02074774 0.0220159  0.04696643 0.01447384\n",
            "  0.03568966 0.0185008  0.03574303 0.03013502 0.02562579 0.02978063\n",
            "  0.09543489 0.04789203 0.01826583 0.02731701 0.00571925 0.06339716\n",
            "  0.02389185 0.02692734 0.05205129 0.11559157 0.03889422 0.02692612\n",
            "  0.04007148 0.03457489]]\n",
            "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wennz2yGKPfv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87760d1b-9a24-4e3f-9271-1fd77323569e"
      },
      "source": [
        "model = model_maker()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / BATCH_SIZE),\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=math.ceil(float(VALIDATION_SAMPLES) / BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 11s 3s/step - loss: 3.8321 - acc: 0.0312 - val_loss: 3.1207 - val_acc: 0.1055\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 10s 3s/step - loss: 3.2279 - acc: 0.1133 - val_loss: 3.0212 - val_acc: 0.2031\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 11s 3s/step - loss: 3.1194 - acc: 0.0977 - val_loss: 2.9075 - val_acc: 0.1211\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 11s 3s/step - loss: 3.0952 - acc: 0.1016 - val_loss: 2.8641 - val_acc: 0.2266\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 9s 2s/step - loss: 3.0631 - acc: 0.0898 - val_loss: 2.8178 - val_acc: 0.2578\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 6s 2s/step - loss: 3.0146 - acc: 0.1500 - val_loss: 2.6906 - val_acc: 0.3594\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 11s 3s/step - loss: 2.9734 - acc: 0.1328 - val_loss: 2.6022 - val_acc: 0.3359\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 9s 2s/step - loss: 2.9234 - acc: 0.1836 - val_loss: 2.4792 - val_acc: 0.3984\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 7s 2s/step - loss: 2.8123 - acc: 0.1562 - val_loss: 2.3337 - val_acc: 0.4570\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 9s 2s/step - loss: 2.7525 - acc: 0.1836 - val_loss: 2.2302 - val_acc: 0.5273\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 6s 2s/step - loss: 2.6638 - acc: 0.1836 - val_loss: 2.0838 - val_acc: 0.6289\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 2.5977 - acc: 0.2600 - val_loss: 1.9257 - val_acc: 0.7500\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 7s 2s/step - loss: 2.4657 - acc: 0.2930 - val_loss: 1.7952 - val_acc: 0.7188\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 8s 2s/step - loss: 2.4139 - acc: 0.2695 - val_loss: 1.7017 - val_acc: 0.7812\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 8s 2s/step - loss: 2.3894 - acc: 0.3125 - val_loss: 1.6587 - val_acc: 0.7852\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 8s 2s/step - loss: 2.3072 - acc: 0.2969 - val_loss: 1.5879 - val_acc: 0.8633\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 7s 2s/step - loss: 2.0778 - acc: 0.4023 - val_loss: 1.4601 - val_acc: 0.8555\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 6s 1s/step - loss: 2.1961 - acc: 0.3398 - val_loss: 1.3500 - val_acc: 0.9023\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 6s 2s/step - loss: 2.0569 - acc: 0.3516 - val_loss: 1.2720 - val_acc: 0.9336\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 6s 1s/step - loss: 2.1484 - acc: 0.3594 - val_loss: 1.2309 - val_acc: 0.9102\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 2.0538 - acc: 0.3789 - val_loss: 1.1597 - val_acc: 0.9609\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 6s 2s/step - loss: 2.0689 - acc: 0.3867 - val_loss: 1.1110 - val_acc: 0.9805\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.9472 - acc: 0.4141 - val_loss: 1.0160 - val_acc: 0.9883\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.9671 - acc: 0.3984 - val_loss: 0.9182 - val_acc: 0.9922\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 6s 1s/step - loss: 1.8570 - acc: 0.4297 - val_loss: 0.8768 - val_acc: 0.9922\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 4s 989ms/step - loss: 1.7356 - acc: 0.4766 - val_loss: 0.8587 - val_acc: 0.9805\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 7s 2s/step - loss: 1.6696 - acc: 0.4805 - val_loss: 0.8361 - val_acc: 0.9844\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.7031 - acc: 0.4961 - val_loss: 0.7988 - val_acc: 0.9648\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.7429 - acc: 0.4922 - val_loss: 0.7312 - val_acc: 0.9961\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.6314 - acc: 0.5117 - val_loss: 0.6650 - val_acc: 0.9922\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.6480 - acc: 0.4844 - val_loss: 0.6161 - val_acc: 0.9844\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.4978 - acc: 0.5586 - val_loss: 0.6114 - val_acc: 0.9961\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 4s 905ms/step - loss: 1.5487 - acc: 0.5000 - val_loss: 0.6008 - val_acc: 0.9961\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 3s 757ms/step - loss: 1.5111 - acc: 0.5500 - val_loss: 0.5566 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.4001 - acc: 0.6094 - val_loss: 0.5087 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.4178 - acc: 0.5938 - val_loss: 0.4707 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.5255 - acc: 0.5500 - val_loss: 0.4687 - val_acc: 0.9844\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.5521 - acc: 0.5312 - val_loss: 0.4760 - val_acc: 0.9766\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.4191 - acc: 0.5586 - val_loss: 0.4765 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.4633 - acc: 0.5781 - val_loss: 0.4892 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.4110 - acc: 0.5938 - val_loss: 0.4816 - val_acc: 0.9961\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.3396 - acc: 0.6367 - val_loss: 0.4100 - val_acc: 0.9961\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 4s 925ms/step - loss: 1.3532 - acc: 0.5664 - val_loss: 0.3504 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 4s 921ms/step - loss: 1.3317 - acc: 0.6133 - val_loss: 0.3334 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.2230 - acc: 0.6562 - val_loss: 0.3236 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 4s 957ms/step - loss: 1.2793 - acc: 0.6367 - val_loss: 0.3160 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 4s 920ms/step - loss: 1.2362 - acc: 0.6172 - val_loss: 0.3154 - val_acc: 0.9961\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.2896 - acc: 0.5781 - val_loss: 0.3284 - val_acc: 0.9922\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 4s 921ms/step - loss: 1.1771 - acc: 0.6758 - val_loss: 0.3000 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 4s 920ms/step - loss: 1.2278 - acc: 0.6172 - val_loss: 0.2666 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 4s 926ms/step - loss: 1.1539 - acc: 0.6484 - val_loss: 0.2470 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 3s 763ms/step - loss: 1.0426 - acc: 0.6750 - val_loss: 0.2448 - val_acc: 0.9961\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 4s 949ms/step - loss: 1.1405 - acc: 0.6523 - val_loss: 0.2339 - val_acc: 0.9961\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 4s 928ms/step - loss: 1.1822 - acc: 0.6133 - val_loss: 0.2299 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 4s 924ms/step - loss: 1.1152 - acc: 0.6641 - val_loss: 0.2329 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 3s 825ms/step - loss: 1.1136 - acc: 0.6367 - val_loss: 0.2276 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 4s 937ms/step - loss: 1.0303 - acc: 0.6836 - val_loss: 0.2145 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 4s 945ms/step - loss: 1.1143 - acc: 0.6100 - val_loss: 0.2138 - val_acc: 0.9961\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 4s 955ms/step - loss: 1.2042 - acc: 0.6211 - val_loss: 0.1981 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.9433 - acc: 0.7422 - val_loss: 0.1867 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 4s 923ms/step - loss: 1.0535 - acc: 0.7148 - val_loss: 0.1777 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 4s 932ms/step - loss: 0.9381 - acc: 0.7227 - val_loss: 0.1763 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 4s 917ms/step - loss: 1.0260 - acc: 0.6875 - val_loss: 0.1655 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.9806 - acc: 0.6992 - val_loss: 0.1682 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 4s 898ms/step - loss: 1.0379 - acc: 0.6602 - val_loss: 0.1664 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1.0981 - acc: 0.6758 - val_loss: 0.1561 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 4s 909ms/step - loss: 0.9704 - acc: 0.6836 - val_loss: 0.1531 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 4s 935ms/step - loss: 0.9106 - acc: 0.7305 - val_loss: 0.1469 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 4s 938ms/step - loss: 0.9520 - acc: 0.6875 - val_loss: 0.1386 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 4s 936ms/step - loss: 1.0034 - acc: 0.6914 - val_loss: 0.1331 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 4s 954ms/step - loss: 0.8992 - acc: 0.7109 - val_loss: 0.1220 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.9321 - acc: 0.6875 - val_loss: 0.1169 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 3s 825ms/step - loss: 0.8217 - acc: 0.7578 - val_loss: 0.1209 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 4s 918ms/step - loss: 0.8967 - acc: 0.7227 - val_loss: 0.1174 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 4s 937ms/step - loss: 0.9043 - acc: 0.7266 - val_loss: 0.1094 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 4s 928ms/step - loss: 0.8510 - acc: 0.7305 - val_loss: 0.1040 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 4s 937ms/step - loss: 0.9377 - acc: 0.7305 - val_loss: 0.1075 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 4s 950ms/step - loss: 0.8506 - acc: 0.7422 - val_loss: 0.1045 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 4s 956ms/step - loss: 0.8844 - acc: 0.7461 - val_loss: 0.0966 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 4s 931ms/step - loss: 0.8400 - acc: 0.7500 - val_loss: 0.0914 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 4s 915ms/step - loss: 0.9515 - acc: 0.7109 - val_loss: 0.0931 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 4s 919ms/step - loss: 0.8127 - acc: 0.7500 - val_loss: 0.0918 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 3s 774ms/step - loss: 0.7506 - acc: 0.7850 - val_loss: 0.0785 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 4s 933ms/step - loss: 0.9153 - acc: 0.7070 - val_loss: 0.0772 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 4s 934ms/step - loss: 0.9213 - acc: 0.7031 - val_loss: 0.0809 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 4s 937ms/step - loss: 0.8204 - acc: 0.7539 - val_loss: 0.0883 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.8264 - acc: 0.7539 - val_loss: 0.0862 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 4s 950ms/step - loss: 0.8674 - acc: 0.7383 - val_loss: 0.0866 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 4s 949ms/step - loss: 0.6985 - acc: 0.7930 - val_loss: 0.0783 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 4s 932ms/step - loss: 0.7368 - acc: 0.8008 - val_loss: 0.0733 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.8274 - acc: 0.7500 - val_loss: 0.0716 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 4s 928ms/step - loss: 0.7691 - acc: 0.7773 - val_loss: 0.0755 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 4s 927ms/step - loss: 0.7731 - acc: 0.7852 - val_loss: 0.0734 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 4s 935ms/step - loss: 0.7792 - acc: 0.7617 - val_loss: 0.0688 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 4s 918ms/step - loss: 0.8829 - acc: 0.7344 - val_loss: 0.0646 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 4s 929ms/step - loss: 0.7053 - acc: 0.7852 - val_loss: 0.0652 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 4s 929ms/step - loss: 0.7708 - acc: 0.7500 - val_loss: 0.0674 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.7765 - acc: 0.7773 - val_loss: 0.0648 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 4s 911ms/step - loss: 0.7325 - acc: 0.7695 - val_loss: 0.0610 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 4s 921ms/step - loss: 0.7018 - acc: 0.7617 - val_loss: 0.0582 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff086591048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSDHgSzQ1zF"
      },
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJMexU6UT6A1"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc2AflYmT8HB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "2411ce1a-bc06-478b-cc1f-813b9524f026"
      },
      "source": [
        "img_path = path_data+'A538.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
        "prediction = model.predict(preprocessed_img)\n",
        "print(prediction)\n",
        "print(validation_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.9541276e-01 4.2292699e-03 2.3507679e-02 4.3859567e-02 1.9467544e-02\n",
            "  6.3630432e-02 2.5264038e-02 4.6668857e-04 2.5347039e-02 1.0730526e-03\n",
            "  2.4110663e-03 3.7078774e-03 3.7987221e-03 1.7847326e-02 1.2594777e-03\n",
            "  5.6591078e-05 2.4122624e-05 2.8368246e-04 8.9046825e-03 1.8888319e-04\n",
            "  7.4729149e-04 4.8159263e-03 2.2629825e-03 6.4583967e-04 5.0580941e-02\n",
            "  2.0657617e-04]]\n",
            "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjqsBU3CT-tp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}